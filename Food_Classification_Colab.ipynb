{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2104acd4",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78665335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageEnhance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, models, datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"‚úÖ Using Device: {device}\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: GPU not enabled!\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b783d",
   "metadata": {},
   "source": [
    "## Step 2: Upload Your Dataset\n",
    "\n",
    "**Option A:** Upload ZIP file (recommended)\n",
    "1. Zip your `food_dataset` folder\n",
    "2. Run the cell below and upload the zip\n",
    "\n",
    "**Option B:** Mount Google Drive\n",
    "- If your dataset is already on Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0dc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION A: Upload ZIP file\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"üì§ Please upload your food_dataset.zip file...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extract\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\nüì¶ Extracting {filename}...\")\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/')\n",
    "    print(\"‚úÖ Extraction complete!\")\n",
    "\n",
    "# Find dataset folder\n",
    "if os.path.exists('/content/food_dataset'):\n",
    "    dataset_path = '/content/food_dataset'\n",
    "elif os.path.exists('/content/food_dataset_augmented'):\n",
    "    dataset_path = '/content/food_dataset_augmented'\n",
    "else:\n",
    "    # List contents to find folder\n",
    "    print(\"\\nüìÇ Contents of /content/:\")\n",
    "    for item in os.listdir('/content/'):\n",
    "        print(f\"   {item}\")\n",
    "    dataset_path = input(\"\\nEnter the dataset folder name: \")\n",
    "    dataset_path = f'/content/{dataset_path}'\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION B: Mount Google Drive (uncomment if using Drive)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# dataset_path = '/content/drive/MyDrive/food_dataset'  # Change path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632bed4",
   "metadata": {},
   "source": [
    "## Step 3: Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìÇ Dataset path: {dataset_path}\\n\")\n",
    "\n",
    "# Get categories\n",
    "categories = [d for d in os.listdir(dataset_path) \n",
    "              if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "categories.sort()\n",
    "\n",
    "print(f\"‚úÖ Found {len(categories)} food categories:\\n\")\n",
    "\n",
    "# Count images\n",
    "data = []\n",
    "total_images = 0\n",
    "for cat in categories:\n",
    "    cat_path = os.path.join(dataset_path, cat)\n",
    "    images = [f for f in os.listdir(cat_path) \n",
    "              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    count = len(images)\n",
    "    total_images += count\n",
    "    data.append({'Category': cat, 'Count': count})\n",
    "    print(f\"  ‚Ä¢ {cat}: {count} images\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total: {total_images} images\")\n",
    "\n",
    "# Visualize distribution\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending=False)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(range(len(df)), df['Count'], color='steelblue')\n",
    "plt.xlabel('Food Category')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Image Distribution Across Categories')\n",
    "plt.xticks(range(len(df)), df['Category'], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82dcdb5",
   "metadata": {},
   "source": [
    "## Step 4: Display Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21fc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples\n",
    "n_samples = min(24, len(categories))\n",
    "fig, axes = plt.subplots(4, 6, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, cat in enumerate(categories[:n_samples]):\n",
    "    cat_path = os.path.join(dataset_path, cat)\n",
    "    images = [f for f in os.listdir(cat_path) \n",
    "              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if images:\n",
    "        img_path = os.path.join(cat_path, images[0])\n",
    "        img = Image.open(img_path).resize((150, 150))\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(cat, fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "for i in range(n_samples, 24):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Images from Each Category', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7d004",
   "metadata": {},
   "source": [
    "## Step 5: Data Augmentation\n",
    "\n",
    "Create augmented images to increase dataset size (7x more images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b905a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageEnhance, ImageFilter\n",
    "\n",
    "# Create augmented dataset folder\n",
    "aug_path = '/content/food_dataset_augmented'\n",
    "os.makedirs(aug_path, exist_ok=True)\n",
    "\n",
    "print(\"üîÑ Augmenting images (7x increase)...\\n\")\n",
    "\n",
    "original_count = 0\n",
    "augmented_count = 0\n",
    "\n",
    "def augment_image(img):\n",
    "    \"\"\"Apply augmentations to image\"\"\"\n",
    "    augmented = []\n",
    "    \n",
    "    # Original\n",
    "    augmented.append(img)\n",
    "    \n",
    "    # Horizontal flip\n",
    "    augmented.append(img.transpose(Image.FLIP_LEFT_RIGHT))\n",
    "    \n",
    "    # Rotation\n",
    "    augmented.append(img.rotate(15))\n",
    "    augmented.append(img.rotate(-15))\n",
    "    \n",
    "    # Brightness\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    augmented.append(enhancer.enhance(1.2))\n",
    "    augmented.append(enhancer.enhance(0.8))\n",
    "    \n",
    "    # Contrast\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    augmented.append(enhancer.enhance(1.2))\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "for cat in categories:\n",
    "    cat_path = os.path.join(dataset_path, cat)\n",
    "    aug_cat_path = os.path.join(aug_path, cat)\n",
    "    os.makedirs(aug_cat_path, exist_ok=True)\n",
    "    \n",
    "    images = [f for f in os.listdir(cat_path) \n",
    "              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    cat_original = len(images)\n",
    "    cat_augmented = 0\n",
    "    \n",
    "    for img_name in images:\n",
    "        try:\n",
    "            img_path = os.path.join(cat_path, img_name)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Get augmented versions\n",
    "            aug_images = augment_image(img)\n",
    "            \n",
    "            # Save all versions\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            for j, aug_img in enumerate(aug_images):\n",
    "                save_path = os.path.join(aug_cat_path, f\"{base_name}_aug{j}.jpg\")\n",
    "                aug_img.save(save_path, 'JPEG', quality=90)\n",
    "                cat_augmented += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    original_count += cat_original\n",
    "    augmented_count += cat_augmented\n",
    "    print(f\"  {cat}: {cat_original} ‚Üí {cat_augmented} images\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üìä AUGMENTATION SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"   Original images: {original_count}\")\n",
    "print(f\"   After augmentation: {augmented_count}\")\n",
    "print(f\"   Increase: {augmented_count - original_count} new images ({(augmented_count/original_count - 1)*100:.0f}% increase)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Update dataset path to use augmented data\n",
    "dataset_path = aug_path\n",
    "print(f\"\\n‚úÖ Using augmented dataset: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f8a0e",
   "metadata": {},
   "source": [
    "## Step 6: Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# IMPORTANT: Use ORIGINAL dataset for proper train/val split\n",
    "# (augmented data causes data leakage - same image variants in train AND val)\n",
    "original_path = '/content/food_dataset'\n",
    "print(f\"üìÇ Using ORIGINAL dataset to prevent data leakage: {original_path}\\n\")\n",
    "\n",
    "# Load dataset without transform first (we'll apply transform in custom datasets)\n",
    "full_dataset = datasets.ImageFolder(original_path, transform=None)\n",
    "class_names = full_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Split 80/20 - this ensures no image overlap between train and val\n",
    "indices = list(range(len(full_dataset)))\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "\n",
    "# Shuffle with fixed seed for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "# Custom dataset to apply different transforms\n",
    "class TransformSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, transform):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = self.indices[idx]\n",
    "        img_path, label = self.dataset.samples[original_idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Create datasets with proper transforms\n",
    "train_dataset = TransformSubset(full_dataset, train_indices, train_transform)\n",
    "val_dataset = TransformSubset(full_dataset, val_indices, val_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "print(f\"‚úÖ Number of classes: {num_classes}\")\n",
    "print(f\"‚úÖ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"‚úÖ Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"\\n‚ö†Ô∏è  Using ORIGINAL data (not augmented) to prevent data leakage!\")\n",
    "print(f\"üìä Online augmentation applied during training only\")\n",
    "print(f\"\\nüìã Classes: {class_names[:5]}... (showing first 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6f8fe",
   "metadata": {},
   "source": [
    "## Step 7: Define Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4fcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, num_classes):\n",
    "    \"\"\"Create a pre-trained model with custom classifier\"\"\"\n",
    "    \n",
    "    if model_name == 'ResNet18':\n",
    "        model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'ResNet50':\n",
    "        model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'EfficientNet-B0':\n",
    "        model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'EfficientNet-B3':\n",
    "        model = models.efficientnet_b3(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'DenseNet121':\n",
    "        model = models.densenet121(weights='IMAGENET1K_V1')\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Models to train\n",
    "model_names = ['ResNet18', 'ResNet50', 'EfficientNet-B0', 'EfficientNet-B3', 'DenseNet121']\n",
    "\n",
    "print(\"üìã Deep Learning Models to train:\")\n",
    "for name in model_names:\n",
    "    print(f\"   ‚Ä¢ {name}\")\n",
    "print(f\"\\n‚úÖ All models use ImageNet pre-trained weights (Transfer Learning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b484c",
   "metadata": {},
   "source": [
    "## Step 8: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=15, lr=0.001):\n",
    "    \"\"\"Train model and return history\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    history = {'train_acc': [], 'val_acc': [], 'train_loss': [], 'val_loss': []}\n",
    "    best_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['val_loss'].append(val_loss / len(val_loader))\n",
    "        \n",
    "        scheduler.step(val_loss / len(val_loader))\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1}/{epochs} - Train: {train_acc:.1f}% - Val: {val_acc:.1f}%\")\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, history, best_acc\n",
    "\n",
    "print(\"‚úÖ Training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0d4a8",
   "metadata": {},
   "source": [
    "## Step 9: Train All Models üöÄ\n",
    "\n",
    "With GPU, this takes about **15-30 minutes** for all 5 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fcfe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "results = {}\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "\n",
    "print(\"üöÄ Starting Deep Learning Training...\\n\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Epochs per model: {EPOCHS}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nüì¶ Training {model_name}...\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(model_name, num_classes)\n",
    "    \n",
    "    # Train\n",
    "    trained_model, history, best_acc = train_model(\n",
    "        model, train_loader, val_loader, epochs=EPOCHS\n",
    "    )\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'model': trained_model,\n",
    "        'history': history,\n",
    "        'best_accuracy': best_acc,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "    \n",
    "    # Save model\n",
    "    safe_name = model_name.replace('-', '_')\n",
    "    save_path = f'/content/models/{safe_name}_food.pth'\n",
    "    torch.save(trained_model.state_dict(), save_path)\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Best Accuracy: {best_acc:.2f}%\")\n",
    "    print(f\"  ‚è±Ô∏è  Time: {train_time/60:.1f} minutes\")\n",
    "    print(f\"  üíæ Saved: {save_path}\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ All models trained in {total_time/60:.1f} minutes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008493e",
   "metadata": {},
   "source": [
    "## Step 10: Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for name, data in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Best Accuracy (%)': data['best_accuracy'],\n",
    "        'Final Train (%)': data['history']['train_acc'][-1],\n",
    "        'Final Val (%)': data['history']['val_acc'][-1],\n",
    "        'Time (min)': data['train_time'] / 60\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values('Best Accuracy (%)', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best model - DEFINE THESE VARIABLES HERE\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_accuracy = comparison_df.iloc[0]['Best Accuracy (%)']\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "colors = ['#2ecc71' if name == best_model_name else '#3498db' for name in comparison_df['Model']]\n",
    "axes[0].barh(comparison_df['Model'], comparison_df['Best Accuracy (%)'], color=colors)\n",
    "axes[0].set_xlabel('Accuracy (%)')\n",
    "axes[0].set_title('Model Comparison - Best Validation Accuracy')\n",
    "axes[0].set_xlim([0, 100])\n",
    "for i, v in enumerate(comparison_df['Best Accuracy (%)']):\n",
    "    axes[0].text(v + 1, i, f'{v:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "# Training curves for best model\n",
    "best_history = results[best_model_name]['history']\n",
    "axes[1].plot(best_history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(best_history['val_acc'], label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title(f'Training History - {best_model_name}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a976633",
   "metadata": {},
   "source": [
    "## Step 11: All Models Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ae98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Classification Report for Best Model\n",
    "# Get best model\n",
    "best_model = results[best_model_name]['model']\n",
    "best_model = best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "# Get predictions\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = best_model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nüìä Classification Report for {best_model_name}:\\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11b: Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name} (Accuracy: {best_accuracy:.1f}%)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aba4c5",
   "metadata": {},
   "source": [
    "## Step 12: Detailed Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Save Best Model and Results\n",
    "# Save best model\n",
    "safe_name = best_model_name.replace('-', '_')\n",
    "final_model_path = f'/content/best_model_{safe_name}.pth'\n",
    "torch.save(best_model.state_dict(), final_model_path)\n",
    "print(f\"‚úÖ Best model saved: {final_model_path}\")\n",
    "\n",
    "# Save class names\n",
    "class_dict = {i: name for i, name in enumerate(class_names)}\n",
    "with open('/content/class_names.json', 'w') as f:\n",
    "    json.dump(class_dict, f, indent=2)\n",
    "print(f\"‚úÖ Class names saved\")\n",
    "\n",
    "# Save results summary\n",
    "summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'best_accuracy': float(best_accuracy),\n",
    "    'num_classes': num_classes,\n",
    "    'image_size': IMG_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'all_results': {name: {\n",
    "        'accuracy': float(data['best_accuracy']), \n",
    "        'time_minutes': float(data['train_time'] / 60)\n",
    "    } for name, data in results.items()}\n",
    "}\n",
    "with open('/content/training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"‚úÖ Training summary saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"üìà Best Accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b51852",
   "metadata": {},
   "source": [
    "## Step 13: Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde7383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Package and Download All Models\n",
    "import shutil\n",
    "\n",
    "# Create output folder\n",
    "output_dir = '/content/trained_models'\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "# Copy files\n",
    "shutil.copy(final_model_path, output_dir)\n",
    "shutil.copy('/content/class_names.json', output_dir)\n",
    "shutil.copy('/content/training_summary.json', output_dir)\n",
    "\n",
    "# Copy all model files from /content/models\n",
    "models_dir = '/content/models'\n",
    "if os.path.exists(models_dir):\n",
    "    for f in os.listdir(models_dir):\n",
    "        shutil.copy(os.path.join(models_dir, f), output_dir)\n",
    "\n",
    "# Create zip\n",
    "shutil.make_archive('/content/food_classification_models', 'zip', output_dir)\n",
    "\n",
    "print(\"‚úÖ Created: food_classification_models.zip\")\n",
    "print(\"\\nüì• Downloading...\")\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('/content/food_classification_models.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c70347a",
   "metadata": {},
   "source": [
    "## Step 14: Download Trained Models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
