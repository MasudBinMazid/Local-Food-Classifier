{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2418b9f7",
   "metadata": {},
   "source": [
    "# Bangladeshi Local Food Classification & Nutrition Suggestion System\n",
    "## University Final Year Thesis Project\n",
    "\n",
    "**Objective:** Classify Bangladeshi local food images and provide nutritional information using Deep Learning\n",
    "\n",
    "**Dataset Structure:** Food images organized in folders by food type (Alu Vorta, Bakorkhani, Bhapa, Burger, Chicken, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## Project Workflow:\n",
    "1. **Data Import & Exploration**\n",
    "2. **Data Preprocessing & Cleaning**\n",
    "3. **Exploratory Data Analysis (EDA)**\n",
    "4. **Data Augmentation**\n",
    "5. **Model Building (Transfer Learning)**\n",
    "6. **Model Training & Evaluation**\n",
    "7. **Model Comparison & Selection**\n",
    "8. **App Development (Gradio/Streamlit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80a1e8",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "**Note:** This notebook is configured for local execution (not Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bc2bbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Make sure you have installed: tensorflow, keras, pillow, matplotlib, seaborn, scikit-learn, pandas, numpy, gradio\n",
      "‚úì You can install them using: pip install tensorflow keras pillow matplotlib seaborn scikit-learn pandas numpy gradio\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install tensorflow keras pillow matplotlib seaborn scikit-learn pandas numpy\n",
    "# !pip install gradio\n",
    "\n",
    "print(\"‚úì Make sure you have installed: tensorflow, keras, pillow, matplotlib, seaborn, scikit-learn, pandas, numpy, gradio\")\n",
    "print(\"‚úì You can install them using: pip install tensorflow keras pillow matplotlib seaborn scikit-learn pandas numpy gradio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3fba127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (10.3.0)\n",
      "Collecting pillow\n",
      "  Downloading pillow-12.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.9/60.9 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: gradio in c:\\users\\user\\anaconda3\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.123.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (2.0.0)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (1.1.6)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (3.11.4)\n",
      "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (2.12.4)\n",
      "Requirement already satisfied: pydub in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio-client==2.0.0->gradio) (2024.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.13.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.66.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.20.0)\n",
      "Requirement already satisfied: rich in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "Downloading pillow-12.0.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.0 MB 960.0 kB/s eta 0:00:08\n",
      "    --------------------------------------- 0.1/7.0 MB 1.6 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.3/7.0 MB 2.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.6/7.0 MB 3.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.8/7.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.0/7.0 MB 3.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.2/7.0 MB 3.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.5/7.0 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.8/7.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.1/7.0 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.3/7.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.4/7.0 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.7/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.9/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.1/7.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.3/7.0 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.5/7.0 MB 4.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.7/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.9/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.3/7.0 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.6/7.0 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.7/7.0 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.8/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.0/7.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.2/7.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.3/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.6/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.7/7.0 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.8/7.0 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.1/7.0 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.2/7.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.4/7.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.5/7.0 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.7/7.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/7.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.0/7.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.7-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/8.1 MB 6.7 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.5/8.1 MB 5.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.8/8.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.1/8.1 MB 5.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.3/8.1 MB 5.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.5/8.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.6/8.1 MB 5.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.9/8.1 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.2/8.1 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.4/8.1 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.5/8.1 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.7/8.1 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.8/8.1 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.9/8.1 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.1/8.1 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.1/8.1 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.2/8.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.5/8.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.5/8.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.7/8.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.0/8.1 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.1/8.1 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/8.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.6/8.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.9/8.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.1/8.1 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.4/8.1 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.4/8.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.3/8.1 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/8.1 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.9/8.1 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.2/8.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.5/8.1 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.7 MB 9.9 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.5/8.7 MB 6.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/8.7 MB 6.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.2/8.7 MB 6.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.4/8.7 MB 6.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.7/8.7 MB 6.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.1/8.7 MB 6.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.4/8.7 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.7/8.7 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.1/8.7 MB 6.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.4/8.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.7/8.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.9/8.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.1/8.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.3/8.7 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.7/8.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.9/8.7 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.2/8.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.5/8.7 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.7/8.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.2/8.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.5/8.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.8/8.7 MB 6.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.2/8.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.5/8.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.9/8.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.2/8.7 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.4/8.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/11.0 MB 9.2 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/11.0 MB 8.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/11.0 MB 6.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/11.0 MB 6.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/11.0 MB 7.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.9/11.0 MB 7.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.2/11.0 MB 7.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.3/11.0 MB 6.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/11.0 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.0/11.0 MB 6.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.3/11.0 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.5/11.0 MB 6.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.9/11.0 MB 6.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.3/11.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.6/11.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/11.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.3/11.0 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.6/11.0 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.0/11.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.8/11.0 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.2/11.0 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.5/11.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.8/11.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.4/11.0 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.9/11.0 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.0/11.0 MB 6.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.6/11.0 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.9/11.0 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.0/11.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.3/11.0 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.9/11.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.0 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 5.8 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, pillow, scikit-learn, pandas, matplotlib, tensorflow\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 2.2.0\n",
      "    Uninstalling threadpoolctl-2.2.0:\n",
      "      Successfully uninstalled threadpoolctl-2.2.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.3.0\n",
      "    Uninstalling pillow-10.3.0:\n",
      "      Successfully uninstalled pillow-10.3.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.4.2\n",
      "    Uninstalling scikit-learn-1.4.2:\n",
      "      Successfully uninstalled scikit-learn-1.4.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.4\n",
      "    Uninstalling matplotlib-3.8.4:\n",
      "      Successfully uninstalled matplotlib-3.8.4\n",
      "Successfully installed matplotlib-3.10.7 pandas-2.3.3 pillow-12.0.0 scikit-learn-1.7.2 tensorflow-2.20.0 threadpoolctl-3.6.0\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è  CRITICAL: RESTART THE KERNEL NOW!  ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n",
      "======================================================================\n",
      "\n",
      "1. Click the circular arrow üîÑ button in the toolbar\n",
      "2. Wait for kernel to restart\n",
      "3. Then run Step 2 (Import Libraries cell)\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\user\\anaconda3\\Lib\\site-packages\\~il'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\user\\anaconda3\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\user\\anaconda3\\Lib\\site-packages\\~atplotlib'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.0 requires pillow<11,>=7.1.0, but you have pillow 12.0.0 which is incompatible.\n",
      "streamlit 1.32.0 requires protobuf<5,>=3.20, but you have protobuf 6.33.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Install packages using pip\n",
    "%pip install --upgrade tensorflow pillow matplotlib seaborn scikit-learn pandas numpy gradio\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è  CRITICAL: RESTART THE KERNEL NOW!  ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. Click the circular arrow üîÑ button in the toolbar\")\n",
    "print(\"2. Wait for kernel to restart\")\n",
    "print(\"3. Then run Step 2 (Import Libraries cell)\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6dd6d",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da58a4be",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Deep Learning Libraries\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\user\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.applications import (\n",
    "    VGG16, VGG19, ResNet50, InceptionV3, MobileNetV2, EfficientNetB0\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"‚úÖ GPU Available: {len(tf.config.list_physical_devices('GPU'))} GPU(s)\")\n",
    "print(f\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba427905",
   "metadata": {},
   "source": [
    "## Step 3: Load Dataset from Local Computer\n",
    "\n",
    "**Instructions:**\n",
    "1. Place your `local_food_pre.zip` file in the same directory as this notebook (G:\\Food\\Code\\)\n",
    "2. Or update the `zip_path` below to point to wherever your ZIP file is located\n",
    "3. The code will extract the ZIP file and organize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c91b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Dataset folder: G:\\Food\\Code\\food_dataset\n",
      "\n",
      "‚úÖ Dataset folder found!\n",
      "\n",
      "üçΩÔ∏è Food categories detected:\n",
      "\n",
      "‚úÖ Total categories: 0\n",
      "‚úÖ Dataset is ready for training!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# ========================================\n",
    "# üìù DATASET FOLDER PATH:\n",
    "# ========================================\n",
    "# The ZIP file has been extracted locally to this folder\n",
    "extract_path = r'G:\\Food\\Code\\food_dataset'\n",
    "\n",
    "print(f\"üîç Python executable: {sys.executable}\")\n",
    "print(f\"üìÇ Dataset folder: {extract_path}\\n\")\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(extract_path):\n",
    "    print(f\"‚ùå ERROR: Dataset folder not found at: {extract_path}\")\n",
    "    print(f\"\\nüí° The folder doesn't exist. Please make sure:\")\n",
    "    print(f\"   1. Extract 'local_food_pre.zip' to 'G:\\\\Food\\\\Code\\\\food_dataset'\")\n",
    "    print(f\"   2. Or update the 'extract_path' variable above\")\n",
    "    print(f\"\\n‚ö†Ô∏è NOTE: If the folder exists on your computer but the notebook can't see it,\")\n",
    "    print(f\"         you may be using a REMOTE kernel. Click the kernel selector in the\")\n",
    "    print(f\"         top-right and choose a LOCAL Python environment.\")\n",
    "else:\n",
    "    try:\n",
    "        # List folders\n",
    "        items = os.listdir(extract_path)\n",
    "        print(f\"‚úÖ Dataset folder found! ({len(items)} items detected)\\n\")\n",
    "        print(\"üçΩÔ∏è Food categories detected:\")\n",
    "        \n",
    "        categories = []\n",
    "        for item in items:\n",
    "            item_path = os.path.join(extract_path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                num_files = len([f for f in os.listdir(item_path) \n",
    "                                if os.path.isfile(os.path.join(item_path, f))])\n",
    "                categories.append((item, num_files))\n",
    "                print(f\"  ‚Ä¢ {item}: {num_files} images\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Total categories: {len(categories)}\")\n",
    "        if len(categories) == 0:\n",
    "            print(\"\\n‚ö†Ô∏è WARNING: Folder exists but no categories found!\")\n",
    "            print(f\"   This usually means the notebook is using a REMOTE kernel\")\n",
    "            print(f\"   that can't access your local G:\\\\ drive.\")\n",
    "            print(f\"\\nüí° SOLUTION: Switch to a LOCAL Python kernel:\")\n",
    "            print(f\"   1. Click the kernel selector in top-right corner\")\n",
    "            print(f\"   2. Select 'Python Environments...'\")\n",
    "            print(f\"   3. Choose your local Python installation\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Dataset is ready for training!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR accessing folder: {e}\")\n",
    "        print(f\"\\nüí° This confirms you're using a REMOTE kernel that can't access local files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99949b0",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze dataset structure\n",
    "def analyze_dataset(dataset_path):\n",
    "    \"\"\"Analyze the dataset structure and image statistics\"\"\"\n",
    "    \n",
    "    # Get all class folders\n",
    "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    classes.sort()\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"DATASET ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nüìä Total Food Classes: {len(classes)}\\n\")\n",
    "    \n",
    "    # Count images per class\n",
    "    class_counts = {}\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        images = [f for f in os.listdir(class_path) \n",
    "                 if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "        class_counts[class_name] = len(images)\n",
    "    \n",
    "    # Create DataFrame for better visualization\n",
    "    df = pd.DataFrame(list(class_counts.items()), columns=['Food Class', 'Image Count'])\n",
    "    df = df.sort_values('Image Count', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìã Images per Food Class:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    total_images = df['Image Count'].sum()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total Images: {total_images}\")\n",
    "    print(f\"Average Images per Class: {total_images/len(classes):.2f}\")\n",
    "    print(f\"Min Images: {df['Image Count'].min()}\")\n",
    "    print(f\"Max Images: {df['Image Count'].max()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return df, classes\n",
    "\n",
    "# Analyze the dataset\n",
    "dataset_df, food_classes = analyze_dataset(extract_path)\n",
    "\n",
    "# Store for later use\n",
    "num_classes = len(food_classes)\n",
    "print(f\"\\n‚úì Found {num_classes} food categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Bar plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(dataset_df['Food Class'], dataset_df['Image Count'], color='steelblue')\n",
    "plt.xlabel('Number of Images', fontsize=12)\n",
    "plt.ylabel('Food Class', fontsize=12)\n",
    "plt.title('Image Distribution Across Food Classes', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Pie chart (top 10 classes)\n",
    "plt.subplot(1, 2, 2)\n",
    "top_10 = dataset_df.head(10)\n",
    "plt.pie(top_10['Image Count'], labels=top_10['Food Class'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Top 10 Food Classes Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = dataset_df['Image Count'].max() / dataset_df['Image Count'].min()\n",
    "print(f\"\\n‚ö†Ô∏è Class Imbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 3:\n",
    "    print(\"  ‚Üí Dataset is imbalanced. Consider using class weights or data augmentation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7aca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image dimensions and formats\n",
    "def analyze_images(dataset_path, sample_size=100):\n",
    "    \"\"\"Analyze image dimensions, formats, and quality\"\"\"\n",
    "    \n",
    "    widths, heights, formats, sizes = [], [], [], []\n",
    "    \n",
    "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    \n",
    "    sample_count = 0\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        images = [f for f in os.listdir(class_path) \n",
    "                 if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "        \n",
    "        for img_name in images[:10]:  # Sample 10 from each class\n",
    "            try:\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                img = Image.open(img_path)\n",
    "                widths.append(img.width)\n",
    "                heights.append(img.height)\n",
    "                formats.append(img.format)\n",
    "                sizes.append(os.path.getsize(img_path) / 1024)  # KB\n",
    "                sample_count += 1\n",
    "                \n",
    "                if sample_count >= sample_size:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if sample_count >= sample_size:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"IMAGE CHARACTERISTICS ANALYSIS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    print(f\"Samples Analyzed: {len(widths)}\")\n",
    "    print(f\"\\nüìê Image Dimensions:\")\n",
    "    print(f\"  Width  - Min: {min(widths)}px, Max: {max(widths)}px, Avg: {np.mean(widths):.0f}px\")\n",
    "    print(f\"  Height - Min: {min(heights)}px, Max: {max(heights)}px, Avg: {np.mean(heights):.0f}px\")\n",
    "    print(f\"\\nüìä Image Formats: {set(formats)}\")\n",
    "    print(f\"\\nüíæ File Sizes:\")\n",
    "    print(f\"  Min: {min(sizes):.2f} KB\")\n",
    "    print(f\"  Max: {max(sizes):.2f} KB\")\n",
    "    print(f\"  Avg: {np.mean(sizes):.2f} KB\")\n",
    "    \n",
    "    return widths, heights\n",
    "\n",
    "widths, heights = analyze_images(extract_path)\n",
    "\n",
    "# Visualize dimensions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(widths, bins=20, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Width (pixels)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Image Width Distribution')\n",
    "\n",
    "axes[1].hist(heights, bins=20, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Height (pixels)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Image Height Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52561df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "def display_sample_images(dataset_path, classes, samples_per_class=3):\n",
    "    \"\"\"Display sample images from each food class\"\"\"\n",
    "    \n",
    "    n_classes = len(classes)\n",
    "    fig, axes = plt.subplots(n_classes, samples_per_class, \n",
    "                             figsize=(15, 3 * n_classes))\n",
    "    \n",
    "    if n_classes == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        images = [f for f in os.listdir(class_path) \n",
    "                 if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "        \n",
    "        for j in range(min(samples_per_class, len(images))):\n",
    "            img_path = os.path.join(class_path, images[j])\n",
    "            img = load_img(img_path)\n",
    "            \n",
    "            if n_classes == 1:\n",
    "                ax = axes[j]\n",
    "            else:\n",
    "                ax = axes[i, j]\n",
    "            \n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            if j == 0:\n",
    "                ax.set_title(f\"{class_name}\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Images from Each Food Class', \n",
    "                 fontsize=16, fontweight='bold', y=1.001)\n",
    "    plt.show()\n",
    "\n",
    "# Display samples (showing first 8 classes to keep it manageable)\n",
    "print(\"\\nüì∏ Displaying sample images from each food class...\\n\")\n",
    "display_sample_images(extract_path, food_classes[:8], samples_per_class=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d486d9",
   "metadata": {},
   "source": [
    "## Step 5: Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check and remove corrupted images\n",
    "def clean_dataset(dataset_path):\n",
    "    \"\"\"Remove corrupted or unreadable images\"\"\"\n",
    "    \n",
    "    print(\"üîç Checking for corrupted images...\\n\")\n",
    "    \n",
    "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    \n",
    "    corrupted_count = 0\n",
    "    total_checked = 0\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        images = [f for f in os.listdir(class_path) \n",
    "                 if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "        \n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            total_checked += 1\n",
    "            \n",
    "            try:\n",
    "                # Try to open and verify the image\n",
    "                img = Image.open(img_path)\n",
    "                img.verify()  # Verify it's a valid image\n",
    "                \n",
    "                # Reopen for further checks (verify() closes the file)\n",
    "                img = Image.open(img_path)\n",
    "                img.load()  # Load the image data\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Corrupted: {class_name}/{img_name}\")\n",
    "                os.remove(img_path)\n",
    "                corrupted_count += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total Images Checked: {total_checked}\")\n",
    "    print(f\"Corrupted Images Removed: {corrupted_count}\")\n",
    "    print(f\"Clean Images: {total_checked - corrupted_count}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return total_checked - corrupted_count\n",
    "\n",
    "clean_image_count = clean_dataset(extract_path)\n",
    "print(\"‚úì Data cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation/test split\n",
    "def create_data_splits(dataset_path, output_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"Split dataset into train, validation, and test sets\"\"\"\n",
    "    \n",
    "    print(f\"üìÇ Creating data splits (Train: {train_ratio*100}%, Val: {val_ratio*100}%, Test: {test_ratio*100}%)...\\n\")\n",
    "    \n",
    "    # Create output directories\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_path, split), exist_ok=True)\n",
    "    \n",
    "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    \n",
    "    split_info = {'train': 0, 'val': 0, 'test': 0}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        images = [f for f in os.listdir(class_path) \n",
    "                 if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "        \n",
    "        # Shuffle images\n",
    "        np.random.shuffle(images)\n",
    "        \n",
    "        # Calculate split points\n",
    "        total = len(images)\n",
    "        train_end = int(total * train_ratio)\n",
    "        val_end = train_end + int(total * val_ratio)\n",
    "        \n",
    "        # Split images\n",
    "        train_images = images[:train_end]\n",
    "        val_images = images[train_end:val_end]\n",
    "        test_images = images[val_end:]\n",
    "        \n",
    "        # Copy images to respective directories\n",
    "        for split, image_list in [('train', train_images), ('val', val_images), ('test', test_images)]:\n",
    "            split_class_path = os.path.join(output_path, split, class_name)\n",
    "            os.makedirs(split_class_path, exist_ok=True)\n",
    "            \n",
    "            for img_name in image_list:\n",
    "                src = os.path.join(class_path, img_name)\n",
    "                dst = os.path.join(split_class_path, img_name)\n",
    "                shutil.copy2(src, dst)\n",
    "                split_info[split] += 1\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train Images: {split_info['train']}\")\n",
    "    print(f\"Validation Images: {split_info['val']}\")\n",
    "    print(f\"Test Images: {split_info['test']}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Create splits\n",
    "split_data_path = r'G:\\Food\\Code\\food_dataset_split'\n",
    "split_data_path = create_data_splits(extract_path, split_data_path)\n",
    "\n",
    "print(\"‚úì Data split completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624bd0db",
   "metadata": {},
   "source": [
    "## Step 6: Data Augmentation & Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image parameters\n",
    "IMG_SIZE = 224  # Standard size for most pre-trained models\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data Augmentation for Training Set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,              # Normalize pixel values\n",
    "    rotation_range=30,            # Random rotation\n",
    "    width_shift_range=0.2,        # Horizontal shift\n",
    "    height_shift_range=0.2,       # Vertical shift\n",
    "    shear_range=0.2,              # Shear transformation\n",
    "    zoom_range=0.2,               # Random zoom\n",
    "    horizontal_flip=True,         # Horizontal flip\n",
    "    brightness_range=[0.8, 1.2],  # Brightness adjustment\n",
    "    fill_mode='nearest'           # Fill mode for new pixels\n",
    ")\n",
    "\n",
    "# Validation and Test Set (only rescaling, no augmentation)\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(split_data_path, 'train'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(split_data_path, 'val'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(split_data_path, 'test'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Store class indices for later use\n",
    "class_indices = train_generator.class_indices\n",
    "class_names = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "print(f\"\\n‚úì Data generators created successfully!\")\n",
    "print(f\"  Training samples: {train_generator.samples}\")\n",
    "print(f\"  Validation samples: {validation_generator.samples}\")\n",
    "print(f\"  Test samples: {test_generator.samples}\")\n",
    "print(f\"  Number of classes: {len(class_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented images\n",
    "def show_augmented_images(generator, num_images=9):\n",
    "    \"\"\"Display augmented images from the generator\"\"\"\n",
    "    \n",
    "    # Get a batch of images\n",
    "    images, labels = next(generator)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(num_images, len(images))):\n",
    "        axes[i].imshow(images[i])\n",
    "        class_idx = np.argmax(labels[i])\n",
    "        axes[i].set_title(f\"{class_names[class_idx]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Augmented Training Images', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nüì∏ Visualizing augmented images...\\n\")\n",
    "show_augmented_images(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238ef15",
   "metadata": {},
   "source": [
    "## Step 7: Model Building with Transfer Learning\n",
    "\n",
    "We'll test multiple pre-trained models to find the best performer:\n",
    "- **MobileNetV2** (lightweight, fast)\n",
    "- **EfficientNetB0** (efficient, good accuracy)\n",
    "- **ResNet50** (deep, robust)\n",
    "- **VGG16** (classic, reliable)\n",
    "- **InceptionV3** (inception modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build transfer learning model\n",
    "def build_transfer_model(base_model_name='MobileNetV2', num_classes=num_classes, \n",
    "                         img_size=IMG_SIZE, trainable_layers=0):\n",
    "    \"\"\"\n",
    "    Build a transfer learning model\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Name of the pre-trained model\n",
    "        num_classes: Number of output classes\n",
    "        img_size: Input image size\n",
    "        trainable_layers: Number of layers to unfreeze (0 = freeze all base layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select base model\n",
    "    if base_model_name == 'MobileNetV2':\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False, \n",
    "                                 input_shape=(img_size, img_size, 3))\n",
    "    elif base_model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, \n",
    "                                    input_shape=(img_size, img_size, 3))\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, \n",
    "                              input_shape=(img_size, img_size, 3))\n",
    "    elif base_model_name == 'VGG16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, \n",
    "                           input_shape=(img_size, img_size, 3))\n",
    "    elif base_model_name == 'InceptionV3':\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, \n",
    "                                 input_shape=(img_size, img_size, 3))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {base_model_name}\")\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Optionally unfreeze last N layers\n",
    "    if trainable_layers > 0:\n",
    "        for layer in base_model.layers[-trainable_layers:]:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    # Build the model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úì Model building function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81a660",
   "metadata": {},
   "source": [
    "## Step 8: Training Callbacks & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329fa682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 30\n",
    "\n",
    "# Create callbacks\n",
    "def get_callbacks(model_name):\n",
    "    \"\"\"Create training callbacks\"\"\"\n",
    "    \n",
    "    # Model checkpoint - save best model\n",
    "    checkpoint_path = rf'G:\\Food\\Code\\models\\best_model_{model_name}.keras'\n",
    "    os.makedirs(r'G:\\Food\\Code\\models', exist_ok=True)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Early stopping - stop if no improvement\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return [checkpoint, early_stop, reduce_lr]\n",
    "\n",
    "print(\"‚úì Training callbacks configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efbfed",
   "metadata": {},
   "source": [
    "## Step 9: Train Multiple Models (Experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3424eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate multiple models\n",
    "models_to_test = ['MobileNetV2', 'EfficientNetB0', 'ResNet50']\n",
    "results = {}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ Training {model_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_transfer_model(base_model_name=model_name)\n",
    "    \n",
    "    # Display model summary\n",
    "    print(f\"\\nüìä Model Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Get callbacks\n",
    "    callbacks = get_callbacks(model_name)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc, test_top3 = model.evaluate(test_generator, verbose=0)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'history': history.history,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_top3_accuracy': test_top3,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ {model_name} Results:\")\n",
    "    print(f\"  Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"  Test Top-3 Accuracy: {test_top3*100:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Save history\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(rf'G:\\Food\\Code\\{model_name}_history.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì All models trained successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0fca6",
   "metadata": {},
   "source": [
    "## Step 10: Model Comparison & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f70162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test Accuracy (%)': [results[m]['test_accuracy']*100 for m in results.keys()],\n",
    "    'Top-3 Accuracy (%)': [results[m]['test_top3_accuracy']*100 for m in results.keys()],\n",
    "    'Test Loss': [results[m]['test_loss'] for m in results.keys()]\n",
    "}).sort_values('Test Accuracy (%)', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MODEL COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot - Test Accuracy\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Test Accuracy (%)'], \n",
    "            color=['#2ecc71', '#3498db', '#e74c3c'][:len(comparison_df)])\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Model Comparison - Test Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0, 100])\n",
    "for i, v in enumerate(comparison_df['Test Accuracy (%)']):\n",
    "    axes[0].text(i, v + 2, f'{v:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Bar plot - Top-3 Accuracy\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['Top-3 Accuracy (%)'], \n",
    "            color=['#2ecc71', '#3498db', '#e74c3c'][:len(comparison_df)])\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Model Comparison - Top-3 Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0, 100])\n",
    "for i, v in enumerate(comparison_df['Top-3 Accuracy (%)']):\n",
    "    axes[1].text(i, v + 2, f'{v:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   Accuracy: {comparison_df.iloc[0]['Test Accuracy (%)']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for best model\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training and validation accuracy/loss\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0].plot(history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    axes[0].plot(history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title(f'{model_name} - Training History (Accuracy)', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[1].plot(history['loss'], label='Train Loss', linewidth=2)\n",
    "    axes[1].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title(f'{model_name} - Training History (Loss)', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for best model\n",
    "print(f\"\\nüìà Training History for Best Model: {best_model_name}\\n\")\n",
    "plot_training_history(results[best_model_name]['history'], best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d77972",
   "metadata": {},
   "source": [
    "## Step 11: Detailed Evaluation (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1dea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "# Generate predictions on test set\n",
    "print(\"üîÆ Generating predictions on test set...\\n\")\n",
    "test_generator.reset()\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# True labels\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(classification_report(true_classes, predicted_classes, \n",
    "                          target_names=list(class_names.values()),\n",
    "                          digits=4))\n",
    "\n",
    "# Overall metrics\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "print(f\"\\n‚úÖ Overall Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=list(class_names.values()),\n",
    "            yticklabels=list(class_names.values()),\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà PER-CLASS ACCURACY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "for i, class_name in class_names.items():\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_accuracy = (class_correct / class_total * 100) if class_total > 0 else 0\n",
    "    print(f\"{class_name:25s} : {class_accuracy:6.2f}% ({class_correct}/{class_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a7ed0",
   "metadata": {},
   "source": [
    "## Step 12: Create Nutrition Database for Bangladeshi Foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9776ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive nutrition database\n",
    "# Note: These are approximate values per 100g serving. Adjust based on your research.\n",
    "\n",
    "nutrition_database = {\n",
    "    'Alu Vorta': {\n",
    "        'description': 'Mashed potato dish with mustard oil, onions, and green chilies',\n",
    "        'calories': 130,\n",
    "        'protein': 2.5,\n",
    "        'carbs': 22,\n",
    "        'fat': 4,\n",
    "        'fiber': 2.5,\n",
    "        'benefits': 'Good source of vitamin C, potassium, and dietary fiber'\n",
    "    },\n",
    "    'Bakorkhani': {\n",
    "        'description': 'Traditional thick, spiced flat-bread',\n",
    "        'calories': 310,\n",
    "        'protein': 8,\n",
    "        'carbs': 52,\n",
    "        'fat': 8,\n",
    "        'fiber': 2,\n",
    "        'benefits': 'Energy-rich, contains carbohydrates and some protein'\n",
    "    },\n",
    "    'Bhapa': {\n",
    "        'description': 'Steamed dish, usually fish or vegetables',\n",
    "        'calories': 150,\n",
    "        'protein': 18,\n",
    "        'carbs': 5,\n",
    "        'fat': 6,\n",
    "        'fiber': 1,\n",
    "        'benefits': 'High in protein, omega-3 fatty acids (if fish)'\n",
    "    },\n",
    "    'Burger': {\n",
    "        'description': 'Fast food sandwich with patty and vegetables',\n",
    "        'calories': 295,\n",
    "        'protein': 15,\n",
    "        'carbs': 28,\n",
    "        'fat': 14,\n",
    "        'fiber': 2,\n",
    "        'benefits': 'Provides protein and energy, moderate in fat'\n",
    "    },\n",
    "    'Chicken': {\n",
    "        'description': 'Chicken curry or preparation',\n",
    "        'calories': 220,\n",
    "        'protein': 27,\n",
    "        'carbs': 3,\n",
    "        'fat': 11,\n",
    "        'fiber': 0.5,\n",
    "        'benefits': 'Excellent source of lean protein, B vitamins'\n",
    "    },\n",
    "    'Chicken Roast': {\n",
    "        'description': 'Roasted or grilled chicken with spices',\n",
    "        'calories': 240,\n",
    "        'protein': 28,\n",
    "        'carbs': 2,\n",
    "        'fat': 13,\n",
    "        'fiber': 0.3,\n",
    "        'benefits': 'High protein, lower carb option'\n",
    "    },\n",
    "    'Chingri Vuna': {\n",
    "        'description': 'Fried prawn dish with spices',\n",
    "        'calories': 180,\n",
    "        'protein': 20,\n",
    "        'carbs': 4,\n",
    "        'fat': 9,\n",
    "        'fiber': 0.5,\n",
    "        'benefits': 'Rich in protein, selenium, and omega-3'\n",
    "    },\n",
    "    'Chomchom': {\n",
    "        'description': 'Traditional sweet made from milk',\n",
    "        'calories': 350,\n",
    "        'protein': 7,\n",
    "        'carbs': 50,\n",
    "        'fat': 14,\n",
    "        'fiber': 0,\n",
    "        'benefits': 'Calcium from milk, quick energy source'\n",
    "    },\n",
    "    'Chowmein': {\n",
    "        'description': 'Stir-fried noodles with vegetables',\n",
    "        'calories': 190,\n",
    "        'protein': 6,\n",
    "        'carbs': 28,\n",
    "        'fat': 6,\n",
    "        'fiber': 3,\n",
    "        'benefits': 'Provides carbohydrates, vegetables add vitamins'\n",
    "    },\n",
    "    'Dal': {\n",
    "        'description': 'Lentil soup/curry',\n",
    "        'calories': 115,\n",
    "        'protein': 9,\n",
    "        'carbs': 20,\n",
    "        'fat': 0.5,\n",
    "        'fiber': 8,\n",
    "        'benefits': 'Excellent protein source, high in fiber and iron'\n",
    "    },\n",
    "    'Egg Curry': {\n",
    "        'description': 'Boiled eggs in spicy curry',\n",
    "        'calories': 210,\n",
    "        'protein': 14,\n",
    "        'carbs': 6,\n",
    "        'fat': 15,\n",
    "        'fiber': 1,\n",
    "        'benefits': 'High quality protein, vitamins A, D, E, B12'\n",
    "    },\n",
    "    'French Fries': {\n",
    "        'description': 'Deep-fried potato strips',\n",
    "        'calories': 312,\n",
    "        'protein': 3.4,\n",
    "        'carbs': 41,\n",
    "        'fat': 15,\n",
    "        'fiber': 3.8,\n",
    "        'benefits': 'Energy from carbs, some potassium'\n",
    "    },\n",
    "    'Fried Chicken': {\n",
    "        'description': 'Deep-fried chicken pieces',\n",
    "        'calories': 320,\n",
    "        'protein': 24,\n",
    "        'carbs': 12,\n",
    "        'fat': 20,\n",
    "        'fiber': 0.5,\n",
    "        'benefits': 'High protein, but higher in fat due to frying'\n",
    "    },\n",
    "    'Fuchka': {\n",
    "        'description': 'Crispy hollow puri with spicy water',\n",
    "        'calories': 140,\n",
    "        'protein': 4,\n",
    "        'carbs': 22,\n",
    "        'fat': 4,\n",
    "        'fiber': 2,\n",
    "        'benefits': 'Low calorie snack, provides quick energy'\n",
    "    },\n",
    "    'Jalebi': {\n",
    "        'description': 'Sweet deep-fried dessert',\n",
    "        'calories': 415,\n",
    "        'protein': 5,\n",
    "        'carbs': 65,\n",
    "        'fat': 16,\n",
    "        'fiber': 0,\n",
    "        'benefits': 'Quick energy from sugar, occasional treat'\n",
    "    },\n",
    "    'Jhalmuri': {\n",
    "        'description': 'Puffed rice snack with spices',\n",
    "        'calories': 325,\n",
    "        'protein': 7,\n",
    "        'carbs': 68,\n",
    "        'fat': 3,\n",
    "        'fiber': 2,\n",
    "        'benefits': 'Light snack, low in fat, provides quick energy'\n",
    "    },\n",
    "    'Kotkoti': {\n",
    "        'description': 'Flaky sweet pastry',\n",
    "        'calories': 450,\n",
    "        'protein': 6,\n",
    "        'carbs': 55,\n",
    "        'fat': 23,\n",
    "        'fiber': 1,\n",
    "        'benefits': 'Energy-dense, occasional treat'\n",
    "    },\n",
    "    'Morog Polao': {\n",
    "        'description': 'Chicken pilaf rice dish',\n",
    "        'calories': 280,\n",
    "        'protein': 18,\n",
    "        'carbs': 38,\n",
    "        'fat': 6,\n",
    "        'fiber': 1.5,\n",
    "        'benefits': 'Balanced meal with protein, carbs, and minimal fat'\n",
    "    },\n",
    "    'Mutton Leg Roast': {\n",
    "        'description': 'Roasted mutton leg with spices',\n",
    "        'calories': 290,\n",
    "        'protein': 26,\n",
    "        'carbs': 2,\n",
    "        'fat': 20,\n",
    "        'fiber': 0.3,\n",
    "        'benefits': 'Rich in protein, iron, and B vitamins'\n",
    "    },\n",
    "    'Paratha': {\n",
    "        'description': 'Layered flatbread',\n",
    "        'calories': 320,\n",
    "        'protein': 6,\n",
    "        'carbs': 42,\n",
    "        'fat': 14,\n",
    "        'fiber': 2,\n",
    "        'benefits': 'Energy from carbs, some protein'\n",
    "    },\n",
    "    'Pera Sondesh': {\n",
    "        'description': 'Milk-based sweet',\n",
    "        'calories': 380,\n",
    "        'protein': 8,\n",
    "        'carbs': 52,\n",
    "        'fat': 16,\n",
    "        'fiber': 0,\n",
    "        'benefits': 'Calcium from milk, protein'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON for later use\n",
    "import json\n",
    "with open(r'G:\\Food\\Code\\nutrition_database.json', 'w') as f:\n",
    "    json.dump(nutrition_database, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Nutrition database created!\")\n",
    "print(f\"   Total food items: {len(nutrition_database)}\")\n",
    "\n",
    "# Display sample\n",
    "sample_food = list(nutrition_database.keys())[0]\n",
    "print(f\"\\nüìä Sample Entry: {sample_food}\")\n",
    "print(json.dumps(nutrition_database[sample_food], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cd60a",
   "metadata": {},
   "source": [
    "## Step 13: Build Gradio Web Application\n",
    "\n",
    "Now let's create an interactive web app where users can upload food images and get:\n",
    "- Food name prediction\n",
    "- Confidence score\n",
    "- Nutritional information\n",
    "- Health benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71dced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Prediction function\n",
    "def predict_food(image):\n",
    "    \"\"\"\n",
    "    Predict food class and return nutritional information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess image\n",
    "        img = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = img_array / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = best_model.predict(img_array, verbose=0)\n",
    "        predicted_class_idx = np.argmax(predictions[0])\n",
    "        confidence = predictions[0][predicted_class_idx] * 100\n",
    "        \n",
    "        # Get class name\n",
    "        predicted_food = class_names[predicted_class_idx]\n",
    "        \n",
    "        # Get top 3 predictions\n",
    "        top_3_idx = np.argsort(predictions[0])[-3:][::-1]\n",
    "        top_3_predictions = []\n",
    "        for idx in top_3_idx:\n",
    "            food_name = class_names[idx]\n",
    "            conf = predictions[0][idx] * 100\n",
    "            top_3_predictions.append(f\"{food_name}: {conf:.2f}%\")\n",
    "        \n",
    "        # Get nutrition info\n",
    "        if predicted_food in nutrition_database:\n",
    "            nutrition = nutrition_database[predicted_food]\n",
    "            \n",
    "            # Create formatted output\n",
    "            result = f\"\"\"\n",
    "üçΩÔ∏è **Detected Food: {predicted_food}**\n",
    "‚úÖ **Confidence: {confidence:.2f}%**\n",
    "\n",
    "üìù **Description:**\n",
    "{nutrition['description']}\n",
    "\n",
    "üìä **Nutritional Information (per 100g):**\n",
    "‚Ä¢ Calories: {nutrition['calories']} kcal\n",
    "‚Ä¢ Protein: {nutrition['protein']} g\n",
    "‚Ä¢ Carbohydrates: {nutrition['carbs']} g\n",
    "‚Ä¢ Fat: {nutrition['fat']} g\n",
    "‚Ä¢ Fiber: {nutrition['fiber']} g\n",
    "\n",
    "üí™ **Health Benefits:**\n",
    "{nutrition['benefits']}\n",
    "\n",
    "üéØ **Top 3 Predictions:**\n",
    "{chr(10).join([f\"{i+1}. {pred}\" for i, pred in enumerate(top_3_predictions)])}\n",
    "\"\"\"\n",
    "        else:\n",
    "            result = f\"\"\"\n",
    "üçΩÔ∏è **Detected Food: {predicted_food}**\n",
    "‚úÖ **Confidence: {confidence:.2f}%**\n",
    "\n",
    "‚ö†Ô∏è Nutrition information not available for this food item.\n",
    "\n",
    "üéØ **Top 3 Predictions:**\n",
    "{chr(10).join([f\"{i+1}. {pred}\" for i, pred in enumerate(top_3_predictions)])}\n",
    "\"\"\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict_food,\n",
    "    inputs=gr.Image(label=\"Upload Food Image\"),\n",
    "    outputs=gr.Textbox(label=\"Prediction & Nutrition Info\", lines=20),\n",
    "    title=\"üáßüá© Bangladeshi Food Classification & Nutrition System\",\n",
    "    description=\"\"\"\n",
    "    Upload an image of Bangladeshi food, and the AI will identify it and provide nutritional information!\n",
    "    \n",
    "    **Supported Foods:** Alu Vorta, Bakorkhani, Bhapa, Chicken, Dal, Paratha, Fuchka, Jalebi, and many more!\n",
    "    \"\"\",\n",
    "    examples=None,  # You can add example images here\n",
    "    theme=\"soft\",\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Launching Gradio App...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92ad72",
   "metadata": {},
   "source": [
    "## Step 14: Save Best Model & Assets for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a models directory to save everything\n",
    "models_dir = r'G:\\Food\\Code\\models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "final_model_path = os.path.join(models_dir, 'bangladeshi_food_classifier_final.keras')\n",
    "best_model.save(final_model_path)\n",
    "print(f\"‚úÖ Best model saved: {final_model_path}\")\n",
    "\n",
    "# Save class names mapping\n",
    "class_names_path = os.path.join(models_dir, 'class_names.json')\n",
    "with open(class_names_path, 'w') as f:\n",
    "    json.dump(class_names, f, indent=2)\n",
    "print(f\"‚úÖ Class names saved: {class_names_path}\")\n",
    "\n",
    "# Save nutrition database\n",
    "nutrition_db_path = os.path.join(models_dir, 'nutrition_database.json')\n",
    "with open(nutrition_db_path, 'w') as f:\n",
    "    json.dump(nutrition_database, f, indent=2)\n",
    "print(f\"‚úÖ Nutrition database saved: {nutrition_db_path}\")\n",
    "\n",
    "# Save results summary\n",
    "summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'test_accuracy': float(results[best_model_name]['test_accuracy']),\n",
    "    'test_top3_accuracy': float(results[best_model_name]['test_top3_accuracy']),\n",
    "    'test_loss': float(results[best_model_name]['test_loss']),\n",
    "    'num_classes': num_classes,\n",
    "    'total_training_samples': train_generator.samples,\n",
    "    'total_validation_samples': validation_generator.samples,\n",
    "    'total_test_samples': test_generator.samples,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'all_models_comparison': {\n",
    "        model: {\n",
    "            'test_accuracy': float(results[model]['test_accuracy']),\n",
    "            'test_top3_accuracy': float(results[model]['test_top3_accuracy']),\n",
    "            'test_loss': float(results[model]['test_loss'])\n",
    "        }\n",
    "        for model in results.keys()\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(models_dir, 'model_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"‚úÖ Model summary saved: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìÅ All files saved to: {models_dir}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(f\"  - {os.path.basename(final_model_path)}\")\n",
    "print(f\"  - {os.path.basename(class_names_path)}\")\n",
    "print(f\"  - {os.path.basename(nutrition_db_path)}\")\n",
    "print(f\"  - {os.path.basename(summary_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c653a16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Next Steps & Recommendations\n",
    "\n",
    "### For Your Thesis:\n",
    "\n",
    "1. **Data Collection Improvements:**\n",
    "   - Collect more images per class (aim for 500+ per class)\n",
    "   - Include variations: different lighting, angles, plates, backgrounds\n",
    "   - Add more Bangladeshi food varieties\n",
    "\n",
    "2. **Model Improvements:**\n",
    "   - Try fine-tuning (unfreeze last layers of base model)\n",
    "   - Experiment with ensemble methods\n",
    "   - Try newer architectures: EfficientNetV2, Vision Transformers\n",
    "\n",
    "3. **Advanced Features:**\n",
    "   - Add serving size estimation\n",
    "   - Implement multi-food detection (if multiple items on plate)\n",
    "   - Add regional food variations\n",
    "\n",
    "4. **Deployment Options:**\n",
    "   - **Gradio** (current) - Easy, shareable link\n",
    "   - **Streamlit** - More customizable UI\n",
    "   - **Mobile App** - Using TensorFlow Lite\n",
    "   - **Web API** - Using FastAPI/Flask\n",
    "\n",
    "5. **Thesis Documentation:**\n",
    "   - Literature review on food classification\n",
    "   - Methodology section (data collection, preprocessing, models)\n",
    "   - Results & Discussion (compare models, confusion matrix analysis)\n",
    "   - Conclusion & Future work\n",
    "\n",
    "### To Run This Notebook:\n",
    "\n",
    "1. **Upload** `local_food_pre.zip` to Google Drive\n",
    "2. **Update** the ZIP path in Step 3\n",
    "3. **Run all cells** sequentially\n",
    "4. **Wait** for training (may take 1-2 hours)\n",
    "5. **Test** the Gradio app with your food images!\n",
    "\n",
    "### Files Generated:\n",
    "- `bangladeshi_food_classifier_final.keras` - Best trained model\n",
    "- `class_names.json` - Class label mappings\n",
    "- `nutrition_database.json` - Nutrition information\n",
    "- `model_summary.json` - Training results summary\n",
    "- Training history CSV files for each model\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your thesis! üéìüáßüá©**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
